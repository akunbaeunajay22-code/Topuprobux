<!doctype html>
<html lang="id">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Robot AI — LEVEL 7 (3D, Voice AI, Wakeword)</title>
<style>
  :root{
    --bg:#030610;
    --panel:#07121a;
    --accent:#55f0ff;
    --muted:#9feff8;
  }
  html,body{height:100%; margin:0; background:linear-gradient(180deg,#02060a,#07101a); color:#dffaff; font-family:Inter,system-ui,Segoe UI,Roboto,Arial;}
  #app{height:100%; display:flex; align-items:center; justify-content:center; padding:16px; box-sizing:border-box;}
  .stage{width:1200px; max-width:98vw; height:820px; border-radius:14px; overflow:hidden; display:grid; grid-template-columns:1fr 420px; gap:12px; box-shadow: 0 30px 90px rgba(0,0,0,0.7); background:linear-gradient(180deg, rgba(255,255,255,0.02), rgba(0,0,0,0.02));}
  /* Left: 3D canvas + overlays */
  #left { position:relative; background:radial-gradient(ellipse at center, rgba(10,25,35,0.2), rgba(0,0,0,0.7)); }
  #three-root{ width:100%; height:100%; display:block; }
  .overlay-controls{ position:absolute; left:12px; top:12px; z-index:70; display:flex; gap:8px; align-items:center; }
  button { background:linear-gradient(180deg,var(--accent),#07b4d9); border:none; color:#002; padding:10px 12px; border-radius:10px; font-weight:700; cursor:pointer; box-shadow:0 8px 22px rgba(2,150,180,0.06); }
  .btn-ghost { background:transparent; color:var(--muted); border:1px solid rgba(125,231,255,0.06); padding:8px 10px; }
  .cinematic{ position:absolute; inset:0; pointer-events:none; mix-blend-mode:screen; opacity:0; transition:opacity .9s;}
  .cinematic.on{ opacity:1; }
  /* Right panel */
  .panel{ padding:14px; display:flex; flex-direction:column; gap:12px; }
  .card{ background:rgba(255,255,255,0.02); padding:10px; border-radius:10px; border:1px solid rgba(255,255,255,0.02); }
  .hud{ font-size:13px; color:#bff8ff; }
  .metrics{ display:flex; gap:8px; flex-wrap:wrap; }
  .pill{ padding:8px 10px; border-radius:8px; background:linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01)); color:#c7fbff; font-weight:600; border:1px solid rgba(125,231,255,0.04); }
  .modes{ display:flex; gap:8px; flex-wrap:wrap; }
  video{ width:100%; height:220px; object-fit:cover; border-radius:8px; background:#000; border:1px solid rgba(255,255,255,0.02); }
  .footer{ font-size:12px; color:#93f1ff88; }
  .controls-voice{ display:flex; gap:8px; align-items:center; }
  input[type=text]{ width:100%; padding:8px; border-radius:8px; border:1px solid rgba(255,255,255,0.04); background:transparent; color:#eaffff; }
  .small{ font-size:12px; color:#9feff8; }
</style>
</head>
<body>
<div id="app">
  <div class="stage">
    <div id="left">
      <div class="overlay-controls">
        <button id="startBtn">Start</button>
        <button id="stopBtn" class="btn-ghost">Stop</button>
        <button id="wakeBtn" class="btn-ghost">Wakeword: ON</button>
        <button id="speakBtn" class="btn-ghost">Speak (TTS)</button>
      </div>

      <div id="three-root"></div>
      <div class="cinematic" id="cinem"></div>
    </div>

    <div class="panel">
      <div class="card hud" id="status">Status: idle • Mode: neutral</div>

      <div class="card">
        <div style="display:flex; justify-content:space-between; align-items:center;">
          <div><strong>Live Metrics</strong></div>
          <div class="small">Level 7</div>
        </div>
        <div class="metrics" style="margin-top:8px;">
          <div class="pill" id="volPill">Vol: 0</div>
          <div class="pill" id="pitchPill">Pitch: 0Hz</div>
          <div class="pill" id="visemePill">Viseme: closed</div>
          <div class="pill" id="learnPill">Learn: neutral 0</div>
        </div>
      </div>

      <div class="card">
        <div style="font-weight:700; margin-bottom:8px;">Camera</div>
        <video id="videoPreview" autoplay muted playsinline></video>
      </div>

      <div class="card">
        <div style="font-weight:700; margin-bottom:8px;">Quick Commands</div>
        <div class="modes">
          <div class="pill" onclick="doCmd('hello')">Hello</div>
          <div class="pill" onclick="doCmd('happy')">Happy</div>
          <div class="pill" onclick="doCmd('angry')">Angry</div>
          <div class="pill" onclick="doCmd('love')">Love</div>
          <div class="pill" onclick="doCmd('glitch')">Glitch</div>
        </div>
      </div>

      <div class="card">
        <div style="font-weight:700; margin-bottom:8px;">Voice Input / Manual</div>
        <div class="controls-voice">
          <input id="manualText" placeholder="Ketik pesan untuk AI..." />
          <button id="aiSend">Send</button>
        </div>
        <div class="small" style="margin-top:8px;">Natural AI uses server endpoint. See code comments.</div>
      </div>

      <div style="flex:1;"></div>
      <div class="footer">Level 7 — 3D face morph • wakeword • natural AI • phoneme-ish lip-sync • gyro • gestures</div>
    </div>
  </div>
</div>

<!-- Libraries -->
<script src="https://unpkg.com/three@0.154.0/build/three.min.js"></script>
<script src="https://unpkg.com/three@0.154.0/examples/js/controls/OrbitControls.js"></script>
<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

<script>
/* ================================
   LEVEL 7 — ALL FEATURES IN ONE
   - 3D face (three.js) with morphable expressions
   - WebAudio lip-sync (6 visemes)
   - SpeechRecognition wakeword + commands
   - Natural AI integration (placeholder / server)
   - Gyro & device tilt, gesture detection (basic)
   - Learning memory via localStorage
   - face-api for expression detection
================================= */

const UI = {
  startBtn: document.getElementById('startBtn'),
  stopBtn: document.getElementById('stopBtn'),
  wakeBtn: document.getElementById('wakeBtn'),
  speakBtn: document.getElementById('speakBtn'),
  status: document.getElementById('status'),
  volPill: document.getElementById('volPill'),
  pitchPill: document.getElementById('pitchPill'),
  visemePill: document.getElementById('visemePill'),
  learnPill: document.getElementById('learnPill'),
  videoPreview: document.getElementById('videoPreview'),
  aiSend: document.getElementById('aiSend'),
  manualText: document.getElementById('manualText'),
  cinem: document.getElementById('cinem')
};

// ---------- state ----------
let isRunning=false;
let stream=null;
let audioCtx=null;
let analyser=null;
let dataArray=null;
let timeArray=null;
let sourceNode=null;
let rafLoop=null;
let faceInterval=null;
let modelsLoaded=false;
let lastFaceAt=Date.now();
let wakewordEnabled=true;
let wakeActive=false;

// learning
const LEARN_KEY = 'robot_level7_counts_v1';
let learnCounts = JSON.parse(localStorage.getItem(LEARN_KEY) || '{}');
function incLearn(label){ learnCounts[label] = (learnCounts[label]||0)+1; localStorage.setItem(LEARN_KEY, JSON.stringify(learnCounts)); updateLearnUI(); }
function updateLearnUI(){ const top = Object.keys(learnCounts).sort((a,b)=>learnCounts[b]-learnCounts[a])[0] || 'neutral'; UI.learnPill.innerText = `Learn: ${top} ${learnCounts[top]||0}`; }
updateLearnUI();

// ---------- three.js scene (3D face) ----------
const container = document.getElementById('three-root');
const scene = new THREE.Scene();
scene.fog = new THREE.FogExp2(0x001018, 0.0028);
const renderer = new THREE.WebGLRenderer({ antialias:true, alpha:true });
renderer.setPixelRatio(Math.min(window.devicePixelRatio,2));
renderer.setSize(container.clientWidth, container.clientHeight);
container.appendChild(renderer.domElement);

const camera = new THREE.PerspectiveCamera(35, container.clientWidth/container.clientHeight, 0.1, 1000);
camera.position.set(0,0.9,3.4);
window.addEventListener('resize', ()=>{ renderer.setSize(container.clientWidth, container.clientHeight); camera.aspect = container.clientWidth/container.clientHeight; camera.updateProjectionMatrix(); });

// lights
const hemi = new THREE.HemisphereLight(0x88f6ff, 0x002233, 0.7); scene.add(hemi);
const point = new THREE.PointLight(0x6ff7ff, 1.6, 12); point.position.set(0,1.5,2); scene.add(point);

const root = new THREE.Group(); scene.add(root);

// head base
const headGeo = new THREE.SphereGeometry(0.98, 96, 96, 0, Math.PI*2, 0, Math.PI*0.9);
headGeo.translate(0,-0.15,0);
const headMat = new THREE.MeshStandardMaterial({ color:0x07121a, metalness:0.25, roughness:0.18, emissive:0x03242b, emissiveIntensity:0.9, transparent:true, opacity:0.98 });
const head = new THREE.Mesh(headGeo, headMat); head.scale.set(1.12,1.12,1.12); root.add(head);

// Hologram shell
const holoMat = new THREE.MeshBasicMaterial({ color:0x6defff, transparent:true, opacity:0.12, blending:THREE.AdditiveBlending, side:THREE.BackSide });
const holo = new THREE.Mesh(headGeo.clone(), holoMat); holo.scale.set(1.06,1.06,1.06); root.add(holo);

// eyes (plane) + inner highlights
const eyeGeo = new THREE.PlaneGeometry(0.42,0.18);
const eyeMat = new THREE.MeshBasicMaterial({ color:0x9ef6ff, transparent:true, opacity:0.98, blending:THREE.AdditiveBlending });
const eyeL = new THREE.Mesh(eyeGeo, eyeMat); eyeL.position.set(-0.36,0.2,0.88); eyeL.rotation.y=-0.05; root.add(eyeL);
const eyeR = eyeL.clone(); eyeR.position.set(0.36,0.2,0.88); eyeR.rotation.y=0.05; root.add(eyeR);

// inner small highlight
const innerEye = new THREE.Mesh(new THREE.PlaneGeometry(0.12,0.06), new THREE.MeshBasicMaterial({ color:0xffffff, transparent:true, opacity:0.14 }));
innerEye.position.set(-0.26,0.255,0.88); root.add(innerEye);
const innerEye2 = innerEye.clone(); innerEye2.position.set(0.26,0.255,0.88); root.add(innerEye2);

// mouth layered
const mouthGeo = new THREE.PlaneGeometry(0.5,0.12);
const mouthA = new THREE.Mesh(mouthGeo, new THREE.MeshBasicMaterial({ color:0x7de7ff, transparent:true, opacity:0.95, blending:THREE.AdditiveBlending }));
const mouthB = new THREE.Mesh(mouthGeo, new THREE.MeshBasicMaterial({ color:0x3bd6ff, transparent:true, opacity:0.4, blending:THREE.AdditiveBlending }));
mouthA.position.set(0,-0.28,0.88); root.add(mouthA); mouthB.position.set(0,-0.28,0.87); mouthB.scale.set(1.05,1.2,1); root.add(mouthB);

// rings & particles
function makeRing(r,o){ const g=new THREE.RingGeometry(r-0.01,r+0.01,128); const m=new THREE.MeshBasicMaterial({ color:0x6fefff, transparent:true, opacity:o, side:THREE.DoubleSide, blending:THREE.AdditiveBlending }); const mesh=new THREE.Mesh(g,m); mesh.rotation.x=Math.PI/2; mesh.position.y=-0.22; scene.add(mesh); return mesh; }
const rings=[ makeRing(1.6,0.03), makeRing(1.9,0.02) ];
const pCount=1100;
const pGeo=new THREE.BufferGeometry();
const pPos=new Float32Array(pCount*3);
for(let i=0;i<pCount;i++){ pPos[i*3+0]=(Math.random()-0.5)*6.2; pPos[i*3+1]=(Math.random()-0.25)*3.6; pPos[i*3+2]=(Math.random()-0.5)*3.6; }
pGeo.setAttribute('position', new THREE.BufferAttribute(pPos,3));
const pMat=new THREE.PointsMaterial({ size:0.009, color:0x69e9ff, transparent:true, opacity:0.9, blending:THREE.AdditiveBlending });
const particles=new THREE.Points(pGeo,pMat); scene.add(particles);

// simple morph "expressions" via small scaling/rotation of facial planes
function applyExpression(label, intensity=0.6){
  switch(label){
    case 'happy':
      eyeMat.color.setHex(0xcfffe8);
      mouthA.material.color && mouthA.material.color.setHex && mouthA.material.color.setHex(0x9effc2);
      mouthA.scale.y = 1.1 + intensity*0.6;
      break;
    case 'angry':
      eyeMat.color.setHex(0xff9a9a);
      mouthA.material.color && mouthA.material.color.setHex && mouthA.material.color.setHex(0xff6a6a);
      mouthA.scale.y = 0.5 + intensity*0.4;
      mouthA.rotation.z = 0.05;
      break;
    case 'love':
      eyeMat.color.setHex(0xff66cc);
      mouthA.material.color && mouthA.material.color.setHex && mouthA.material.color.setHex(0xff66cc);
      mouthA.scale.y = 1.6;
      break;
    case 'glitch':
      eyeMat.color.setHex(0x00ffea);
      break;
    case 'sleepAI':
      eyeMat.color.setHex(0x006699);
      mouthA.scale.y = 0.2;
      break;
    default:
      eyeMat.color.setHex(0x9ef6ff);
      mouthA.scale.y = 1;
      mouthA.rotation.z = 0;
      break;
  }
  // record learning
  incLearn(label);
  UI.status.innerText = `Status: running • Mode: ${label}`;
}

// ---------- audio (WebAudio) for lip-sync + pitch ----------
async function initAudio(){
  if (audioCtx) return true;
  try{
    audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    // request mic only here when start called (we get full stream later)
    return true;
  }catch(e){ console.warn('audio init', e); return false; }
}

// helper: compute RMS + simple pitch (auto-correlation)
function analyzeAudioFromStream(stream){
  if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  sourceNode = audioCtx.createMediaStreamSource(stream);
  analyser = audioCtx.createAnalyser();
  analyser.fftSize = 2048;
  sourceNode.connect(analyser);
  dataArray = new Uint8Array(analyser.frequencyBinCount);
  timeArray = new Float32Array(analyser.fftSize);
}

function computeAudioFeatures(){
  if (!analyser) return {rms:0,pitch:0};
  analyser.getByteTimeDomainData(dataArray);
  // RMS
  let sum=0;
  for(let i=0;i<dataArray.length;i++){ let v=(dataArray[i]-128)/128; sum+=v*v; }
  const rms = Math.sqrt(sum/dataArray.length);
  // autocorrelation pitch coarse
  analyser.getFloatTimeDomainData(timeArray);
  let SIZE = timeArray.length;
  let bestOffset=-1, bestCorr=0;
  for(let offset=20; offset<1000 && offset<SIZE; offset++){
    let corr=0;
    for(let i=0;i<SIZE-offset;i++) corr += Math.abs(timeArray[i]-timeArray[i+offset]);
    corr = 1 - (corr/(SIZE-offset));
    if (corr > bestCorr){ bestCorr=corr; bestOffset=offset; }
  }
  let pitch=0;
  if (bestCorr > 0.45 && bestOffset>0) pitch = Math.round((audioCtx.sampleRate||44100)/bestOffset);
  return {rms, pitch};
}

// 6-stage visemes mapping (0..5)
function visemeFromRMS(rms, pitch){
  if (rms < 0.003) return 0;
  if (rms < 0.01) return 1;
  if (rms < 0.02) return 2;
  if (rms < 0.035) return 3;
  if (rms < 0.07) return 4;
  return 5;
}
function setViseme(stage){
  // map Stage to mouth scale for 3D planes
  const scales = [0.18, 0.45, 0.85, 1.2, 1.6, 2.2];
  const s = scales[Math.max(0,Math.min(scales.length-1,stage))];
  mouthA.scale.y = s;
  mouthB.scale.y = s*1.1;
  UI.visemePill.innerText = `Viseme: ${stage}`;
}

// ---------- face-api (optional) ----------
async function loadFaceModels(){
  if (modelsLoaded) return;
  try{
    UI.status.innerText = 'Loading face models...';
    const MODEL_URL = 'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights';
    await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
    await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
    modelsLoaded = true;
    UI.status.innerText = 'Models loaded';
  }catch(e){ console.warn('face model load fail', e); modelsLoaded=false; UI.status.innerText='Models failed'; }
}

// ---------- camera & audio start/stop ----------
async function startAll(){
  if (isRunning) return;
  try{
    stream = await navigator.mediaDevices.getUserMedia({ video:{ facingMode:'user', width:640, height:480 }, audio:true });
    UI.videoPreview.srcObject = stream;
    analyzeAudioFromStream(stream);
    await loadFaceModels().catch(()=>{/*ignore*/});
    lastFaceAt = Date.now();
    isRunning = true;
    startLoops();
    cinematicIntro();
    UI.status.innerText = 'Status: running • Mode: neutral';
  }catch(e){
    console.error('start failed', e);
    UI.status.innerText = 'Start failed: permission?';
  }
}
function stopAll(){
  isRunning = false;
  if (stream) { stream.getTracks().forEach(t=>t.stop()); stream=null; }
  if (audioCtx){ try{ audioCtx.close(); }catch(e){} audioCtx=null; analyser=null; sourceNode=null; }
  if (faceInterval){ clearInterval(faceInterval); faceInterval=null; }
  if (rafLoop){ cancelAnimationFrame(rafLoop); rafLoop=null; }
  UI.status.innerText = 'Status: stopped';
}

// cinematic flash
function cinematicIntro(){
  UI.cinem.classList.add('on');
  setTimeout(()=>UI.cinem.classList.remove('on'), 1100);
}

/* ---------- face loop: detect face & expressions ---------- */
function startFaceLoop(){
  if (!modelsLoaded || !isRunning) return;
  const v = UI.videoPreview;
  faceInterval = setInterval(async ()=>{
    try{
      const options = new faceapi.TinyFaceDetectorOptions({ inputSize:128, scoreThreshold:0.55 });
      const res = await faceapi.detectSingleFace(v, options).withFaceExpressions();
      if (res){
        lastFaceAt = Date.now();
        // expression
        let best='neutral', bv=0;
        for (let k in res.expressions) if (res.expressions[k] > bv){ bv=res.expressions[k]; best=k; }
        // map
        const map = { happy:'happy', angry:'angry', sad:'sad', surprised:'surprise', neutral:'neutral', disgusted:'angry', fearful:'angry' };
        const mode = map[best] || 'neutral';
        applyExpression(mode, bv);
        // distance heuristic
        const h = res.detection.box.height;
        if (h > v.videoHeight * 0.38) applyExpression('surprise', 0.9);
        // face center -> eye follow
        const box = res.detection.box;
        const cx = box.x + box.width/2, cy = box.y + box.height/2;
        const nx = (cx / v.videoWidth)*2 - 1, ny = (cy / v.videoHeight)*2 - 1;
        eyeL.position.x = THREE.MathUtils.lerp(eyeL.position.x, -0.36 + nx*0.06, 0.12);
        eyeR.position.x = THREE.MathUtils.lerp(eyeR.position.x, 0.36 + nx*0.06, 0.12);
        eyeL.position.y = THREE.MathUtils.lerp(eyeL.position.y, 0.2 - ny*0.03, 0.12);
        eyeR.position.y = THREE.MathUtils.lerp(eyeR.position.y, 0.2 - ny*0.03, 0.12);
      } else {
        // relax
        eyeL.position.x = THREE.MathUtils.lerp(eyeL.position.x, -0.36, 0.06);
        eyeR.position.x = THREE.MathUtils.lerp(eyeR.position.x, 0.36, 0.06);
        eyeL.position.y = THREE.MathUtils.lerp(eyeL.position.y, 0.2, 0.06);
        eyeR.position.y = THREE.MathUtils.lerp(eyeR.position.y, 0.2, 0.06);
      }
    }catch(e){ console.warn('face loop err', e); }
  }, 120);
}

/* ---------- analysis loop: audio -> viseme, heuristics ---------- */
function startLoops(){
  function loop(){
    if (!isRunning) return;
    // audio features
    const af = computeAudioFeatures();
    const vis = visemeFromRMS(af.rms, af.pitch);
    setViseme(vis);
    UI.volPill.innerText = `Vol: ${af.rms.toFixed(3)}`;
    UI.pitchPill.innerText = `Pitch: ${af.pitch||0}Hz`;

    // if no face detection active, heuristics
    if (!modelsLoaded || !faceInterval){
      if (af.rms < 0.003) applyExpression('sleepAI',0.2);
      else if (af.rms < 0.02) applyExpression('happy',0.4);
      else if (af.rms < 0.05) applyExpression('happy',0.7);
      else applyExpression('overheat',0.9);
    }

    // auto sleep
    if (Date.now() - lastFaceAt > 6000) applyExpression('sleepAI',0.2);

    // animate rings & particles
    const t = performance.now() * 0.001;
    rings.forEach((r,i)=> r.rotation.z = t*0.02*(i+1));
    const pos = pGeo.attributes.position.array;
    for (let i=0;i<pCount;i++){ let idx=i*3+1; pos[idx]+=Math.sin(t*0.4 + i)*0.0008; if (pos[idx] > 3) pos[idx]=-1.6; if (pos[idx] < -1.6) pos[idx]=3; }
    pGeo.attributes.position.needsUpdate = true;

    rafLoop = requestAnimationFrame(loop);
  }
  loop();
  // start face loop too if models available
  if (modelsLoaded) startFaceLoop();
}

/* ---------- wakeword & speech recognition ---------- */
/* We use Web Speech API (SpeechRecognition). Wakeword detection is simple:
   the recognizer runs continuously; when it hears the wakeword it activates "conversation mode".
   For production-grade wakeword use a wakeword engine (Porcupine, Snowboy, Vosk, etc.) */
window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
const recognizer = new window.SpeechRecognition();
recognizer.lang = 'id-ID';
recognizer.interimResults = false;
recognizer.continuous = true;

recognizer.onresult = async (ev) => {
  const spoken = ev.results[ev.resultIndex][0].transcript.toLowerCase().trim();
  console.log('spk:', spoken);
  // wakeword "hey dex" or "hey dexlaza"
  if (wakewordEnabled && (spoken.includes('hey dex') || spoken.includes('hey dexlaza') || spoken.includes('halo robot'))){
    wakeActive = true;
    UI.status.innerText = 'Wake detected — listening...';
    speakTextLocal('Ya? Saya aktif.','happy');
    return;
  }
  // if in wake mode or wakeword disabled, process commands / passthrough to AI
  if (wakeActive || !wakewordEnabled){
    // check command map
    if (processCommands(spoken)) return;
    // otherwise send to natural AI
    const reply = await queryNaturalAI(spoken);
    if (reply) { speakTextLocal(reply, 'normal'); applyExpression('happy',0.5); }
    wakeActive = false; // require wake for next explicit conversation
  } else {
    // ignore casual speech
    console.log('ignored (no wake)');
  }
};
recognizer.onerror = (e)=> console.warn('rec error', e);
recognizer.onend = ()=> { try{ recognizer.start(); }catch(e){} };
recognizer.start();

/* ---------- command mapping ---------- */
const commands = {
  'hello': ()=> speakTextLocal('Halo!', 'happy'),
  'mode happy': ()=> applyExpression('happy',0.8),
  'mode angry': ()=> applyExpression('angry',0.9),
  'mode love': ()=> applyExpression('love',0.9),
  'mode glitch': ()=> applyExpression('glitch',0.9),
  'scan': ()=> speakTextLocal('Memulai pemindaian area','robotic'),
  'who are you': ()=> speakTextLocal('Saya robot level tujuh.','normal'),
  'reset learning': ()=> { learnCounts={}; localStorage.removeItem(LEARN_KEY); updateLearnUI(); speakTextLocal('Pembelajaran direset','normal'); }
};
function processCommands(text){
  for (const k in commands){ if (text.includes(k)) { commands[k](); return true; } }
  return false;
}

/* ---------- Natural AI integration (placeholder) ----------
  This function expects you to host a server endpoint that wraps a large language model
  (ChatGPT/OpenAI, or your custom model). Example endpoint: POST /api/ai
  Request body: { prompt: "<user text>" }
  Response: { reply: "<text>" }
  For TTS premium: you can also provide /api/tts that returns audio binary.
------------------------------------------------------------ */
async function queryNaturalAI(userText){
  UI.status.innerText = 'Querying AI...';
  try{
    // <<=== IMPORTANT: Replace URL with your server endpoint that calls ChatGPT or LLM ===>>
    const res = await fetch('/api/ai', {
      method:'POST',
      headers:{ 'Content-Type':'application/json' },
      body: JSON.stringify({ prompt: userText })
    });
    if (!res.ok) throw new Error('AI server error');
    const data = await res.json();
    UI.status.innerText = 'AI replied';
    return data.reply || 'Maaf, tidak ada jawaban.';
  }catch(e){
    console.warn('AI query failed', e);
    UI.status.innerText = 'AI failed';
    return 'Maaf, koneksi AI gagal.';
  }
}

/* ---------- TTS: Uses server premium TTS if available, otherwise WebSpeech fallback ----------
  For premium TTS:
  - POST /api/tts with { text:"...", voice:"alloy", style:"emotional" }
  - server returns audio (wav/mp3) or streaming URL
  This code checks /api/tts and falls back to speechSynthesis.
------------------------------------------------------------ */
async function speakTextLocal(text, emotion='normal'){
  // try server premium TTS first
  try{
    const res = await fetch('/api/tts', {
      method:'POST',
      headers:{ 'Content-Type':'application/json' },
      body: JSON.stringify({ text, emotion })
    });
    if (res.ok){
      // server returns audio blob
      const blob = await res.blob();
      const url = URL.createObjectURL(blob);
      const audio = new Audio(url);
      audio.onplay = ()=> { /* optionally animate mouth */ };
      audio.onended = ()=> { setViseme(0); };
      // connect to AudioContext for lip-sync: create MediaElementSource
      if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const src = audioCtx.createMediaElementSource(audio);
      if (!analyser){ analyser = audioCtx.createAnalyser(); analyser.fftSize = 2048; }
      src.connect(analyser);
      analyser.connect(audioCtx.destination);
      audio.play();
      return;
    }
  }catch(e){ console.warn('premium tts failed', e); }

  // fallback to built-in speechSynthesis
  if (!('speechSynthesis' in window)) { console.warn('No TTS'); return; }
  const utt = new SpeechSynthesisUtterance(text);
  utt.lang = 'id-ID';
  utt.rate = (emotion==='angry')?1.1:1;
  utt.pitch = (emotion==='happy')?1.2:1;
  utt.onstart = ()=> { /* animate mouth randomly */;
    const iv = setInterval(()=> {
      const r = Math.random();
      if (r < 0.3) setViseme(0);
      else if (r < 0.6) setViseme(2);
      else setViseme(4);
    }, 80);
    utt.onend = ()=> { clearInterval(iv); setViseme(0); };
  };
  speechSynthesis.speak(utt);
}

/* ---------- wakeword toggle ---------- */
UI.wakeBtn.addEventListener('click', ()=>{ wakewordEnabled = !wakewordEnabled; UI.wakeBtn.innerText = `Wakeword: ${wakewordEnabled?'ON':'OFF'}`; });

/* ---------- manual AI send ---------- */
UI.aiSend.addEventListener('click', async ()=>{
  const txt = UI.manualText.value.trim();
  if (!txt) return;
  const reply = await queryNaturalAI(txt);
  speakTextLocal(reply,'normal');
});

/* ---------- device orientation (gyro) ---------- */
window.addEventListener('deviceorientation', (ev)=>{
  if (!isRunning) return;
  const g = ev.gamma || 0;
  const b = ev.beta || 0;
  root.rotation.y = THREE.MathUtils.lerp(root.rotation.y, THREE.MathUtils.degToRad(g*0.6), 0.08);
  root.rotation.x = THREE.MathUtils.lerp(root.rotation.x, THREE.MathUtils.degToRad(b*0.12 - 6), 0.06);
});

/* ---------- render loop ---------- */
function renderLoop(){
  requestAnimationFrame(renderLoop);
  const t = performance.now() * 0.001;
  head.rotation.y = THREE.MathUtils.lerp(head.rotation.y, Math.sin(t*0.6)*0.03, 0.02);
  head.position.y = Math.sin(t*0.8)*0.015;
  renderer.render(scene, camera);
}
renderLoop();

/* ---------- UI buttons ---------- */
UI.startBtn.addEventListener('click', ()=> startAll());
UI.stopBtn.addEventListener('click', ()=> stopAll());
UI.speakBtn.addEventListener('click', ()=> speakTextLocal('Halo! Ini suara alami dari robot Level 7.', 'happy'));

/* ---------- start helper for page load (optional) ---------- */
function startAll(){
  if (isRunning) return startAllAsync();
  return startAllAsync();
}
async function startAllAsync(){
  await initAudio();
  try{
    stream = await navigator.mediaDevices.getUserMedia({ video:{ facingMode:'user', width:640, height:480 }, audio:true });
    UI.videoPreview.srcObject = stream;
    analyzeAudioFromStream(stream);
    await loadFaceModels().catch(()=>{/*ignore*/});
    lastFaceAt = Date.now();
    isRunning = true;
    startLoops();
    cinematicIntro();
    UI.status.innerText = 'Status: running • Mode: neutral';
  }catch(e){
    console.error('start fail',e);
    UI.status.innerText = 'Start failed (permissions?)';
  }
}

/* ---------- expose quick command helper ---------- */
function doCmd(name){
  const map = {
    'hello': ()=> speakTextLocal('Halo!', 'happy'),
    'happy': ()=> applyExpression('happy',0.8),
    'angry': ()=> applyExpression('angry',0.9),
    'love': ()=> applyExpression('love',0.9),
    'glitch': ()=> applyExpression('glitch',0.9)
  };
  if (map[name]) map[name]();
}

/* ---------- safety unload ---------- */
window.addEventListener('beforeunload', ()=> stopAll());

/* ---------- final note ----------
 - To make Natural AI and Premium TTS work, implement server endpoints:
   POST /api/ai  -> { prompt }  returns { reply }
   POST /api/tts -> { text, voice, emotion } returns audio blob
 - If you want, aku bisa bantu buatkan server Node.js (Express) yang
   calls OpenAI + ElevenLabs (or other) and exposes /api/ai and /api/tts.
---------------------------------- */

</script>
</body>
</html>