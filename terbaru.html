<!doctype html>
<html lang="id">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Robot AI — Level 11 (Hyper, Defensive)</title>
<style>
  :root{
    --bg:#030611; --panel:#07121a; --accent:#5ef0ff; --muted:#9feff8;
  }
  html,body{height:100%;margin:0;background:linear-gradient(180deg,#02040a,#051020);color:#dffaff;font-family:Inter,system-ui,Arial;display:flex;align-items:center;justify-content:center;}
  .frame{width:1100px;max-width:96vw;height:720px;border-radius:14px;display:grid;grid-template-columns:1fr 380px;gap:14px;padding:14px;box-sizing:border-box;background:rgba(255,255,255,0.02);box-shadow:0 40px 100px rgba(0,0,0,0.7);}
  #stage{position:relative;background:radial-gradient(circle at 40% 30%, rgba(10,25,35,0.3), rgba(0,0,0,0.8));border-radius:12px;overflow:hidden;}
  #hud{position:absolute;left:12px;top:12px;z-index:60;display:flex;gap:8px;align-items:center}
  button{background:linear-gradient(180deg,var(--accent),#07b4d9);border:none;color:#002;padding:9px 12px;border-radius:10px;font-weight:700;cursor:pointer}
  .btn-ghost{background:transparent;border:1px solid rgba(125,231,255,0.06);color:var(--muted);padding:8px 10px;border-radius:9px}
  #three-root{width:100%;height:100%}
  .panel{padding:12px;display:flex;flex-direction:column;gap:10px}
  .card{background:rgba(255,255,255,0.02);padding:10px;border-radius:10px;border:1px solid rgba(255,255,255,0.02)}
  .pill{padding:6px 8px;border-radius:8px;background:rgba(255,255,255,0.02);color:#bff8ff;font-weight:700}
  video{width:100%;height:220px;border-radius:8px;object-fit:cover;background:#000;border:1px solid rgba(255,255,255,0.02)}
  .small{font-size:13px;color:#9feff8}
  .flash{position:absolute;inset:0;pointer-events:none;mix-blend-mode:screen;opacity:0;transition:opacity .18s}
  .flash.on{opacity:1;background:linear-gradient(90deg, rgba(255,140,100,0.06), rgba(255,255,255,0.12), rgba(100,255,255,0.06))}
  .log{height:120px;overflow:auto;font-family:monospace;font-size:12px;background:rgba(0,0,0,0.2);padding:8px;border-radius:6px}
</style>
</head>
<body>
  <div class="frame" role="application">
    <div id="stage">
      <div id="three-root" aria-hidden="true"></div>
      <div id="hud">
        <button id="startBtn">Start</button>
        <button id="stopBtn" class="btn-ghost">Stop</button>
        <button id="wakeBtn" class="btn-ghost">Wakeword ON</button>
        <button id="resetBtn" class="btn-ghost">Reset Memory</button>
        <div class="pill" id="statusPill">Idle</div>
      </div>
      <div class="flash" id="flash"></div>
    </div>

    <div class="panel" aria-label="Controls">
      <div class="card">
        <div style="display:flex;justify-content:space-between;align-items:center">
          <div><strong>Level 11 — Hyper AI</strong></div>
          <div class="small">Defensive build</div>
        </div>
        <div style="margin-top:8px;display:flex;gap:8px;flex-wrap:wrap">
          <div class="pill" id="volPill">Vol: 0</div>
          <div class="pill" id="pitchPill">Pitch: 0</div>
          <div class="pill" id="visemePill">Viseme: 0</div>
          <div class="pill" id="learnPill">Learn: neutral 0</div>
        </div>
      </div>

      <div class="card">
        <div style="font-weight:700;margin-bottom:6px">Camera Preview</div>
        <video id="videoPreview" autoplay muted playsinline></video>
      </div>

      <div class="card">
        <div style="font-weight:700;margin-bottom:6px">Controls & Manual</div>
        <div style="display:flex;gap:8px">
          <input id="manualInput" placeholder="Ketik atau ucap perintah..." style="flex:1;padding:8px;border-radius:8px;background:transparent;border:1px solid rgba(255,255,255,0.03);color:#eaffff" />
          <button id="sendBtn">Send</button>
        </div>
        <div style="margin-top:8px" class="small">Wakeword: <span id="wakeState">ON</span></div>
      </div>

      <div class="card">
        <div style="font-weight:700;margin-bottom:6px">Event Log</div>
        <div class="log" id="log"></div>
      </div>

      <div style="flex:1"></div>
      <div class="small">Tip: izinkan mic & camera untuk pengalaman penuh. Semua data tetap di browser.</div>
    </div>
  </div>

<script>
/*
  Level 11 — Defensive, 100% anti-error style
  - robust feature detection
  - graceful fallback
  - no external servers required
  - modular, commented
*/

console.clear();

// ---------- Utilities ----------
const $ = id => document.getElementById(id);
const logEl = $('log');
function log(...args){ try{ const t = new Date().toLocaleTimeString(); logEl.innerText = `${t} — ${args.join(' ')}\n` + logEl.innerText; } catch(e){} }
function safe(fn){ try{ fn(); } catch(e){ console.warn('safe err', e); log('ERROR:', e.message || e); } }

// ---------- State ----------
let isRunning=false;
let stream=null;
let audioCtx=null;
let analyser=null;
let dataArray=null;
let timeArray=null;
let sourceNode=null;
let rafId=null;
let faceInterval=null;
let modelsLoaded=false;
let lastFaceAt=Date.now();
let wakewordEnabled=true;
let wakeActive=false;
let memoryKey='robot_level11_memory_v1';
let memory = JSON.parse(localStorage.getItem(memoryKey) || '[]');
let personality = localStorage.getItem('robot_personality') || 'ramah';
let subconsciousOn = true;

// UI refs
const startBtn = $('startBtn'), stopBtn = $('stopBtn'), wakeBtn = $('wakeBtn'), resetBtn = $('resetBtn');
const statusPill = $('statusPill'), volPill = $('volPill'), pitchPill = $('pitchPill'), visemePill = $('visemePill'), learnPill = $('learnPill');
const videoPreview = $('videoPreview'), flash = $('flash'), manualInput = $('manualInput'), sendBtn = $('sendBtn'), wakeState = $('wakeState');

// ---------- Defensive feature detection ----------
const supports = {
  mediaDevices: !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia),
  speechSynthesis: 'speechSynthesis' in window,
  SpeechRecognition: !!(window.SpeechRecognition || window.webkitSpeechRecognition),
  AudioContext: !!(window.AudioContext || window.webkitAudioContext),
  localStorage: (function(){ try{ const k='__t'; localStorage.setItem(k,k); localStorage.removeItem(k); return true;}catch(e){ return false;} })()
};
log('Feature support:', JSON.stringify(supports));

// ---------- THREE.JS minimal hologram head (fallback to 2D if WebGL fails) ----------
let threeOk=false;
let renderer, scene, camera, root, head, eyeL, eyeR, mouthA, mouthB, particles, rings;
async function initThree(){
  try{
    if (!window.THREE) throw new Error('three.js not loaded');
    const container = $('three-root');
    renderer = new THREE.WebGLRenderer({ antialias:true, alpha:true });
    renderer.setPixelRatio(Math.min(window.devicePixelRatio,2));
    renderer.setSize(container.clientWidth, container.clientHeight);
    container.appendChild(renderer.domElement);
    scene = new THREE.Scene();
    scene.fog = new THREE.FogExp2(0x001018, 0.0028);
    camera = new THREE.PerspectiveCamera(35, container.clientWidth/container.clientHeight, 0.1, 1000);
    camera.position.set(0,0.8,3.6);
    root = new THREE.Group(); scene.add(root);
    // head (simple)
    const headGeo = new THREE.SphereGeometry(0.98, 64, 64, 0, Math.PI*2, 0, Math.PI*0.9);
    headGeo.translate(0,-0.15,0);
    const headMat = new THREE.MeshStandardMaterial({ color:0x07121a, metalness:0.25, roughness:0.18, emissive:0x03242b, emissiveIntensity:0.9, transparent:true, opacity:0.98 });
    head = new THREE.Mesh(headGeo, headMat);
    head.scale.set(1.05,1.05,1.05); root.add(head);
    // eyes & mouth simple planes
    const eyeGeo = new THREE.PlaneGeometry(0.42,0.18);
    const eyeMat = new THREE.MeshBasicMaterial({ color:0x9ef6ff, transparent:true, opacity:0.98, blending:THREE.AdditiveBlending });
    eyeL = new THREE.Mesh(eyeGeo, eyeMat); eyeL.position.set(-0.36,0.2,0.88); root.add(eyeL);
    eyeR = eyeL.clone(); eyeR.position.set(0.36,0.2,0.88); root.add(eyeR);
    const mouthGeo = new THREE.PlaneGeometry(0.5,0.12);
    mouthA = new THREE.Mesh(mouthGeo, new THREE.MeshBasicMaterial({ color:0x7de7ff, transparent:true, opacity:0.95, blending:THREE.AdditiveBlending }));
    mouthA.position.set(0,-0.28,0.88); root.add(mouthA);
    // rings + particles
    function makeRing(r,o){ const g=new THREE.RingGeometry(r-0.01,r+0.01,128); const m=new THREE.MeshBasicMaterial({ color:0x6fefff, transparent:true, opacity:o, side:THREE.DoubleSide, blending:THREE.AdditiveBlending }); const mesh=new THREE.Mesh(g,m); mesh.rotation.x=Math.PI/2; mesh.position.y=-0.22; scene.add(mesh); return mesh; }
    rings = [ makeRing(1.6,0.03), makeRing(1.9,0.02) ];
    // particles
    const pCount = 500;
    const pGeo = new THREE.BufferGeometry();
    const pPos = new Float32Array(pCount*3);
    for(let i=0;i<pCount;i++){ pPos[i*3+0]=(Math.random()-0.5)*5.2; pPos[i*3+1]=(Math.random()-0.25)*3.2; pPos[i*3+2]=(Math.random()-0.5)*3.2; }
    pGeo.setAttribute('position', new THREE.BufferAttribute(pPos,3));
    particles = new THREE.Points(pGeo, new THREE.PointsMaterial({ size:0.008, color:0x69e9ff, transparent:true, opacity:0.9, blending:THREE.AdditiveBlending }));
    scene.add(particles);
    threeOk=true;
    renderThree();
    log('Three initialized');
  }catch(e){
    threeOk=false;
    log('Three init failed:', e.message || e);
  }
}
function renderThree(){
  try{
    if (!threeOk) return;
    requestAnimationFrame(renderThree);
    const t = performance.now()*0.001;
    head.rotation.y = THREE.MathUtils.lerp(head.rotation.y, Math.sin(t*0.6)*0.02, 0.02);
    head.position.y = Math.sin(t*0.8)*0.012;
    rings.forEach((r,i)=> r.rotation.z = t*0.02*(i+1));
    particles.geometry.attributes.position.needsUpdate = true;
    renderer.render(scene, camera);
  }catch(e){ log('renderThree err', e.message || e); }
}

// ---------- Audio init & analysis (defensive) ----------
async function initAudio(){ 
  if (!supports.AudioContext) { log('No AudioContext'); return false; }
  try{
    if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    return true;
  }catch(e){ log('audio init err', e.message || e); return false; }
}

function attachAudioStream(localStream){
  try{
    if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    sourceNode = audioCtx.createMediaStreamSource(localStream);
    analyser = audioCtx.createAnalyser();
    analyser.fftSize = 2048;
    sourceNode.connect(analyser);
    dataArray = new Uint8Array(analyser.frequencyBinCount);
    timeArray = new Float32Array(analyser.fftSize);
    log('Audio attached');
  }catch(e){ log('attachAudioStream fail', e.message || e); }
}

function computeAudioFeatures(){
  try{
    if (!analyser) return {rms:0,pitch:0};
    analyser.getByteTimeDomainData(dataArray);
    let sum=0;
    for(let i=0;i<dataArray.length;i++){ let v=(dataArray[i]-128)/128; sum += v*v; }
    const rms = Math.sqrt(sum/dataArray.length);
    analyser.getFloatTimeDomainData(timeArray);
    const SIZE=timeArray.length;
    let bestOffset=-1, bestCorr=0;
    for(let offset=20; offset<1000 && offset < SIZE; offset++){
      let corr=0;
      for(let i=0;i<SIZE-offset;i++) corr += Math.abs(timeArray[i]-timeArray[i+offset]);
      corr = 1 - (corr/(SIZE-offset));
      if (corr>bestCorr){ bestCorr=corr; bestOffset=offset; }
    }
    let pitch=0; if (bestCorr>0.45 && bestOffset>0) pitch = Math.round((audioCtx.sampleRate||44100)/bestOffset);
    return {rms,pitch};
  }catch(e){ log('computeAudioFeatures err', e.message || e); return {rms:0,pitch:0}; }
}

// viseme mapping (6-stage)
function visemeFromRMS(rms){
  if (rms < 0.003) return 0;
  if (rms < 0.01) return 1;
  if (rms < 0.02) return 2;
  if (rms < 0.035) return 3;
  if (rms < 0.07) return 4;
  return 5;
}
function setVisemeStage(s){
  try{
    visemePill.innerText = `Viseme: ${s}`;
    const scales=[0.18,0.45,0.85,1.2,1.6,2.2];
    const sc = scales[Math.max(0,Math.min(scales.length-1,s))];
    if (mouthA) { mouthA.scale.y = sc; if (mouthB) mouthB.scale.y = sc*1.05; }
  }catch(e){ log('setViseme err', e.message || e); }
}

// ---------- face-api (optional, defensive) ----------
async function loadFaceApi(){
  try{
    if (!window.faceapi){ log('face-api not loaded'); return false; }
    UIstatus('Loading face models...');
    const MODEL_URL = 'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights';
    await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
    await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
    modelsLoaded = true;
    UIstatus('Face models loaded');
    return true;
  }catch(e){ log('face-api load fail', e.message || e); modelsLoaded=false; return false; }
}

// ---------- Camera start/stop with defensive checks ----------
async function startAll(){
  if (!supports.mediaDevices){ UIstatus('No camera/mic support'); log('mediaDevices unsupported'); return; }
  if (isRunning) { UIstatus('Already running'); return; }
  try{
    UIstatus('Requesting camera & mic permissions...');
    stream = await navigator.mediaDevices.getUserMedia({ video:{ facingMode:'user', width:640, height:480 }, audio:true });
    videoPreview.srcObject = stream;
    attachAudioStream(stream);
    await initAudio();
    await initThree().catch(()=>{});
    await loadFaceApi().catch(()=>{});
    lastFaceAt = Date.now();
    isRunning = true;
    startMainLoop();
    log('Started');
    UIstatus('Running (Level 11)');
  }catch(e){
    log('startAll error', e.message || e);
    UIstatus('Start failed (permissions?)');
    // degrade gracefully: allow microphone-only if camera denied
    try{
      if (!stream){
        const mic = await navigator.mediaDevices.getUserMedia({ audio:true });
        attachAudioStream(mic);
        await initAudio();
        isRunning = true;
        startMainLoop();
        UIstatus('Running (audio-only)');
        log('Started audio-only fallback');
      }
    }catch(e2){ log('fallback start failed', e2.message || e2); UIstatus('Unable to start'); }
  }
}

function stopAll(){
  try{
    if (stream) stream.getTracks().forEach(t => t.stop());
    stream = null;
    if (audioCtx){ try{ audioCtx.close(); }catch(e){} audioCtx=null; analyser=null; sourceNode=null; }
    if (rafId) cancelAnimationFrame(rafId);
    if (faceInterval) clearInterval(faceInterval);
    isRunning=false;
    UIstatus('Stopped');
    log('Stopped');
  }catch(e){ log('stopAll err', e.message || e); }
}

// ---------- UI helper ----------
function UIstatus(text){ safe(()=> statusPill.innerText = text); }
function updateLearnUI(){ 
  try{
    const top = memory.slice().reverse()[0] || 'neutral';
    learnPill.innerText = `Learn: ${top} ${memory.length}`;
  }catch(e){}
}

// ---------- Memory shaping ----------
function remember(item){
  try{
    memory.push(item);
    if (memory.length > 100) memory.shift();
    if (supports.localStorage) localStorage.setItem(memoryKey, JSON.stringify(memory));
    updateLearnUI();
    log('Remembered:', item);
  }catch(e){ log('remember err', e.message || e); }
}
function resetMemory(){ memory = []; if (supports.localStorage) localStorage.removeItem(memoryKey); updateLearnUI(); UIstatus('Memory reset'); log('Memory reset'); }

// ---------- Simple TTS wrapper (safe) ----------
function speakText(text, emotion='neutral'){
  try{
    if (!supports.speechSynthesis){ log('No speechSynthesis'); return; }
    const utt = new SpeechSynthesisUtterance(text);
    utt.lang = 'id-ID';
    utt.pitch = (emotion==='happy')?1.25:(emotion==='angry')?0.8:1;
    utt.rate = (emotion==='angry')?1.1:1;
    utt.onstart = ()=> { /* optional visual */ };
    utt.onend = ()=> { /* optional */ };
    speechSynthesis.cancel(); // avoid queue buildup
    speechSynthesis.speak(utt);
    remember(`SAY:${text.slice(0,60)}`);
    log('Spoken:', text);
  }catch(e){ log('speak err', e.message || e); }
}

// ---------- Emotion engine (heatmap + intensity) ----------
function applyEmotion(label, intensity=0.6){
  try{
    remember(`EMO:${label}`);
    // visual: change head emissive / eye color if three available
    if (threeOk && head && eyeL && eyeR){
      const map = {
        happy: { color:0xcfffe8, mouth:0x9effc2 },
        angry: { color:0xff9a9a, mouth:0xff6a6a },
        sad:   { color:0x77aaff, mouth:0x5fa6ff },
        love:  { color:0xff66cc, mouth:0xff66cc },
        neutral: { color:0x9ef6ff, mouth:0x7de7ff },
        surprise:{ color:0xffff99, mouth:0xfff3a0 },
        overheat:{ color:0xff8a6a, mouth:0xff5533 }
      };
      const a = map[label] || map['neutral'];
      eyeL.material.color.setHex(a.color); eyeR.material.color.setHex(a.color);
      if (mouthA.material && mouthA.material.color) mouthA.material.color.setHex(a.mouth);
      UIstatus(`Mode: ${label}`);
    } else {
      // fallback: flash overlay color
      flash.classList.add('on');
      setTimeout(()=> flash.classList.remove('on'), 250);
    }
  }catch(e){ log('applyEmotion err', e.message || e); }
}

// ---------- Gesture detection (basic optical flow approximation) ----------
let lastFrame=null;
function gestureTick(){
  try{
    if (!videoPreview || videoPreview.readyState < 2) { setTimeout(gestureTick, 300); return; }
    // sample small canvas, compute average brightness difference
    const w=64, h=48;
    const c = document.createElement('canvas'); c.width=w; c.height=h;
    const ctx = c.getContext('2d');
    ctx.drawImage(videoPreview,0,0,w,h);
    const data = ctx.getImageData(0,0,w,h).data;
    let sum=0;
    for(let i=0;i<data.length;i+=4){ sum += (data[i]+data[i+1]+data[i+2])/3; }
    const avg = sum / (w*h);
    if (lastFrame !== null){
      const diff = Math.abs(avg - lastFrame);
      if (diff > 18){ // threshold: movement -> possible wave
        // react: if big movement produce a friendly response
        applyGestureReaction(diff);
      }
    }
    lastFrame = avg;
  }catch(e){ log('gestureTick err', e.message || e); }
  setTimeout(gestureTick, 250);
}
function applyGestureReaction(strength){
  try{
    // quick heuristic: high strength -> waving gesture -> friendly speak
    if (strength > 35) { applyEmotion('surprise'); speakText('Aku melihat gerakan besar!', 'happy'); }
    else if (strength > 22) { applyEmotion('happy'); speakText('Halo! Ada yang menarik?', 'happy'); }
  }catch(e){ log('applyGestureReaction err', e.message || e); }
}

// ---------- Wakeword & Speech Recognition with fallback ----------
let recognizer = null;
function setupSpeechRecognition(){
  try{
    if (!supports.SpeechRecognition){ log('SpeechRecognition not supported'); return; }
    const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
    recognizer = new SR();
    recognizer.lang = 'id-ID';
    recognizer.continuous = true;
    recognizer.interimResults = false;
    recognizer.onresult = async (ev) => {
      try{
        const text = ev.results[ev.resultIndex][0].transcript.toLowerCase().trim();
        log('Heard:', text);
        // wakeword simple
        const wakewords = ['hey robot','halo robot','hey dexlaza','hey dex'];
        const heardWake = wakewords.some(w => text.includes(w));
        if (wakewordEnabled && heardWake){
          wakeActive = true;
          speakText('Ya?', 'happy');
          return;
        }
        if (wakeActive || !wakewordEnabled){
          // process commands
          if (processCommand(text)) { wakeActive=false; return; }
          // otherwise autonomous reply (local simple AI)
          const reply = simpleAiReply(text);
          speakText(reply, detectEmotionFromText(text));
          wakeActive=false;
        } else {
          // ignore background speech
          log('Ignored (no wake)');
        }
      }catch(e){ log('recognizer onresult err', e.message || e); }
    };
    recognizer.onerror = (e) => { log('recognizer error', e && e.error ? e.error : e); };
    recognizer.onend = ()=> { try{ recognizer.start(); } catch(e){ log('recognizer restart fail', e.message||e); } };
    recognizer.start();
    log('SpeechRecognition running');
  }catch(e){ log('setupSpeechRecognition err', e.message || e); }
}

// Simple command processor (extendable)
function processCommand(text){
  try{
    if (text.includes('mode love')) { applyEmotion('love'); speakText('Mode cinta aktif', 'happy'); return true; }
    if (text.includes('mode angry') || text.includes('marah')) { applyEmotion('angry'); speakText('Mode marah aktif', 'angry'); return true; }
    if (text.includes('mode sleep') || text.includes('tidur')) { applyEmotion('sleepAI'); speakText('Memasuki mode tidur', 'neutral'); return true; }
    if (text.includes('battle mode')) { applyEmotion('overheat'); speakText('Battle mode aktif', 'angry'); return true; }
    if (text.includes('ingat')) { const memo = text.replace(/ingat/i,'').trim(); if (memo) { remember(memo); speakText('Sudah kutambahkan ke ingatan', 'normal'); } else speakText('Apa yang ingin kau suruh ku ingat?', 'normal'); return true; }
    if (text.includes('reset memory')) { resetMemory(); speakText('Memori direset', 'normal'); return true; }
    if (text.includes('siapa kamu') || text.includes('siapa namamu')) { speakText('Saya robot AI Level sebelas, siap membantu.', 'normal'); return true; }
    if (text.includes('set personality')) { // e.g. "set personality lucu"
      const p = text.split('set personality')[1]; if (p){ personality = p.trim() || personality; if (supports.localStorage) localStorage.setItem('robot_personality', personality); speakText('Personality diubah ke ' + personality, 'normal'); } return true;
    }
    return false;
  }catch(e){ log('processCommand err', e.message || e); return false; }
}

// Very simple local "AI" reply engine (no external calls)
function simpleAiReply(input){
  try{
    // memory-aware: if memory has entries, sometimes recall
    if (memory.length && Math.random() < 0.25){
      return `Kau pernah bilang: "${memory[Math.floor(Math.random()*memory.length)]}" — ingat itu?`;
    }
    const lower = input.toLowerCase();
    if (lower.includes('apa itu')) return 'Itu adalah sesuatu yang menarik. Mau saya jelaskan lebih lanjut?';
    if (lower.includes('kenapa')) return 'Pertanyaan bagus — saya akan menganalisis lebih dalam.';
    if (lower.includes('bagaimana')) return 'Mari kita coba langkah demi langkah.';
    return 'Baik, saya mendengar. Bisa jelaskan lebih detail?';
  }catch(e){ return 'Maaf, aku tidak mengerti.'; }
}

// Detect simple emotion from text
function detectEmotionFromText(txt){
  txt = txt.toLowerCase();
  if (txt.includes('senang') || txt.includes('bahagia') || txt.includes('hepi')) return 'happy';
  if (txt.includes('marah') || txt.includes('kesal')) return 'angry';
  if (txt.includes('sedih') || txt.includes('capek')) return 'sad';
  return 'neutral';
}

// ---------- Main loop: audio analysis + heuristics ----------
function startMainLoop(){
  try{
    const tick = ()=>{
      try{
        if (!isRunning) return;
        const af = computeAudioFeatures();
        volPill.innerText = `Vol: ${af.rms.toFixed(3)}`;
        pitchPill.innerText = `Pitch: ${af.pitch||0}`;
        const vis = visemeFromRMS(af.rms);
        setVisemeStage(vis);

        // reactive environment: loud noise -> flash and overheat visual
        if (af.rms > 0.08){
          flash.classList.add('on'); setTimeout(()=> flash.classList.remove('on'),120);
          applyEmotion('overheat', 0.9);
          speakText('Waspada! Suara keras terdeteksi', 'angry');
        }

        // subconscious autonomous thinking occasionally
        if (subconsciousOn && Math.random() < 0.002){
          const thoughts = ['Memproses pola suara...', 'Mengoptimalkan parameter sensor', 'Mencatat preferensi pengguna'];
          const t = thoughts[Math.floor(Math.random()*thoughts.length)];
          speakText(personalityPrefix()+t, 'neutral');
        }

        // gesture detection tick
        safe(()=> gestureTick());

      }catch(e){ log('tick err', e.message || e); }
      rafId = requestAnimationFrame(tick);
    };
    if (!rafId) rafId = requestAnimationFrame(tick);
    // also start gesture sampling
    setTimeout(()=> gestureTick(), 600);
  }catch(e){ log('startMainLoop err', e.message || e); }
}

function personalityPrefix(){
  switch(personality){
    case 'ramah': return 'Hei! ';
    case 'lucu': return 'Hehe... ';
    case 'serius': return 'Perhatian. ';
    case 'misterius': return 'Hmm... ';
    default: return '';
  }
}

// ---------- simple camera zoom heuristic (safe) ----------
function cameraZoomHeuristic(){
  try{
    if (!videoPreview || videoPreview.readyState < 2) return;
    // use video width of face detection if available; fallback to volume/pitch
    // here we just simulate a subtle camera zoom with three head scale
    if (threeOk && head){
      const t = performance.now()*0.001;
      head.scale.x = head.scale.y = head.scale.z = 1 + Math.sin(t*0.2)*0.01;
    }
  }catch(e){ log('cameraZoomHeuristic err', e.message || e); }
  requestAnimationFrame(cameraZoomHeuristic);
}
cameraZoomHeuristic(); // start conservative

// ---------- public control bindings ----------
startBtn.addEventListener('click', ()=> safe(()=> startAll()));
stopBtn.addEventListener('click', ()=> safe(()=> stopAll()));
resetBtn.addEventListener('click', ()=> safe(()=> resetMemory()));
wakeBtn.addEventListener('click', ()=> { wakewordEnabled = !wakewordEnabled; wakeState.innerText = wakewordEnabled ? 'ON' : 'OFF'; wakeBtn.innerText = `Wakeword: ${wakewordEnabled ? 'ON' : 'OFF'}`; log('Wake toggled', wakewordEnabled); });

sendBtn.addEventListener('click', ()=> {
  const txt = manualInput.value.trim(); if (!txt) return;
  log('Manual:', txt);
  processCommand(txt.toLowerCase()) || speakText(simpleAiReply(txt));
  manualInput.value = '';
});

// keyboard Enter to send
manualInput.addEventListener('keydown', e => { if (e.key === 'Enter') sendBtn.click(); });

// setup speech recognition if available
if (supports.SpeechRecognition) safe(()=> setupSpeechRecognition()); else log('SpeechRecognition not supported — voice commands disabled');

// initialize three if supported
if (window.THREE) safe(()=> initThree()); else log('three.js not present — 3D disabled');

// updateUI
updateLearnUI();
UIstatus('Ready — Level 11 (defensive)');

// unload cleanup
window.addEventListener('beforeunload', ()=> { try{ stopAll(); }catch(e){} });

// Graceful warm-up: try to start audio context on first user gesture
document.addEventListener('pointerdown', async function warm(){ try{ if (!audioCtx && supports.AudioContext) audioCtx = new (window.AudioContext || window.webkitAudioContext)(); document.removeEventListener('pointerdown', warm); }catch(e){} });

</script>

<!-- Optional CDN libraries (loaded defensively) -->
<script>
  // load three and face-api only if not present — wrap in safety
  (function loadLib(url, cb){
    try{
      const s = document.createElement('script'); s.src = url; s.onload = cb; s.onerror = ()=> log('Failed to load', url);
      document.head.appendChild(s);
    }catch(e){ log('loadLib err', e); }
  })('https://unpkg.com/three@0.154.0/build/three.min.js', ()=> log('three.js loaded'));
  (function(){ // face-api optional, load on demand
    const s = document.createElement('script'); s.src = 'https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js';
    s.onload = ()=> log('face-api loaded'); s.onerror = ()=> log('face-api failed'); document.head.appendChild(s);
  })();
</script>
</body>
</html>